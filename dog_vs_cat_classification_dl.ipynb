{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NqRhq6TCu9AU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "\n",
    "#training\n",
    "import tensorflow as tf\n",
    "\n",
    "#Image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aflKh_ElQbsE",
    "outputId": "fe17e00b-9396-4f00-c1c3-aed73ad21413"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/tongpython/cat-and-dog?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 218M/218M [00:20<00:00, 11.2MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/ansinha/.cache/kagglehub/datasets/tongpython/cat-and-dog/versions/1\n"
     ]
    }
   ],
   "source": [
    "#import data from kaggle\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"tongpython/cat-and-dog\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qApeqIcMNH-q",
    "outputId": "1aed052e-8f14-4c0b-c609-732db8a1c2de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cat files: 4001\n",
      "Number of dog files: 4006\n"
     ]
    }
   ],
   "source": [
    "# counting files\n",
    "path_to_cat_folder = path+\"/training_set/training_set/cats\"\n",
    "path_to_dog_folder =path+\"/training_set/training_set/dogs\"\n",
    "\n",
    "path_to_cat_folder, dirs, cat_files = next(os.walk(path_to_cat_folder))\n",
    "file_count_cat = len(cat_files)\n",
    "print( \"Number of cat files:\", file_count_cat)\n",
    "\n",
    "path_to_dog_folder, dirs, dog_files = next(os.walk(path_to_dog_folder))\n",
    "file_count_dog = len(dog_files)\n",
    "print(\"Number of dog files:\", file_count_dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "setFzyenUaY2",
    "outputId": "abdb0b04-eaa6-429b-e9dd-d26d9162a81c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created directories\n",
      "Skipped: _DS_Store\n",
      "Cat Files resizing and grayscaling Done\n"
     ]
    }
   ],
   "source": [
    "#Processing cat images\n",
    "os.makedirs(path+'/working/cats/', exist_ok=True)\n",
    "os.makedirs(path+'/working/dogs/', exist_ok=True)\n",
    "print(f\"Successfully created directories\")\n",
    "\n",
    "\n",
    "for filename in cat_files:\n",
    "  if filename == '_DS_Store':\n",
    "    print(\"Skipped:\", filename)\n",
    "    continue\n",
    "      \n",
    "\n",
    "\n",
    "    \n",
    "  image_cat = Image.open(path_to_cat_folder + \"/\" + filename)\n",
    "  resized_cat_image = image_cat.resize((224, 224))\n",
    "\n",
    "  resized_cat_image.save(path+'/working/cats/' + filename)\n",
    "\n",
    "print(\"Cat Files resizing and grayscaling Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hr51_bpaHPq",
    "outputId": "30640ea8-4a80-4626-8f09-013ca506ac61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped: _DS_Store\n",
      "Dog Files resizing and grayscaling Done\n"
     ]
    }
   ],
   "source": [
    "#Processing dog images\n",
    "for filename in dog_files:\n",
    "  if filename == '_DS_Store':\n",
    "    print(\"Skipped:\", filename)\n",
    "    continue\n",
    "\n",
    "  image_dog = Image.open(path_to_dog_folder + \"/\" + filename)\n",
    "  resized_dog_image = image_dog.resize((224, 224))\n",
    "  resized_dog_image.save(path+'/working/dogs/' + filename)\n",
    "\n",
    "print(\"Dog Files resizing and grayscaling Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ogR7K55Jkzt0",
    "outputId": "cfd39348-f7b4-4998-909e-d1b2dac85e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X and Y created\n"
     ]
    }
   ],
   "source": [
    "#creating X, y, X_test, y_test\n",
    "\n",
    "cat_training_filenames = os.listdir(path+\"/working/cats\")\n",
    "dog_training_filenames = os.listdir(path+\"/working/dogs\")\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for filename in cat_training_filenames:\n",
    "  if filename == '_DS_Store':\n",
    "    print(\"Skipped:\", filename)\n",
    "    continue\n",
    "  X.append(np.array(Image.open(path+\"/working/cats/\" + filename)))\n",
    "  Y.append(0)\n",
    "\n",
    "for filename in dog_training_filenames:\n",
    "  if filename == '_DS_Store':\n",
    "    print(\"Skipped:\", filename)\n",
    "    continue\n",
    "  X.append(np.array(Image.open(path+\"/working/dogs/\" + filename)))\n",
    "  Y.append(1)\n",
    "\n",
    "print(\"X and Y created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvQfLzrugbJg"
   },
   "source": [
    "#Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "p3_Sq_aQ21Yn"
   },
   "outputs": [],
   "source": [
    "random_indices = np.random.permutation(len(X))\n",
    "X = np.array(X)[random_indices]\n",
    "Y = np.array(Y)[random_indices]\n",
    "\n",
    "X=X/255 # rescaling\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Qj-9zpw_hcPd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8005, 224, 224, 3)\n",
      "(6404, 224, 224, 3)\n",
      "(6404,)\n",
      "(1601, 224, 224, 3)\n",
      "(1601,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BV738EAftyIZ"
   },
   "source": [
    "#Building the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "raJ7d2Ylqoe2"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Uv8TYUHix9Sa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,562</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │         \u001b[38;5;34m2,562\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,260,546</span> (8.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,260,546\u001b[0m (8.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,562</span> (10.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,562\u001b[0m (10.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mobilenet_V2_model = \"https://www.kaggle.com/models/google/mobilenet-v2/TensorFlow2/035-128-classification/2\"\n",
    "# pretrained_model = hub.KerasLayer(mobilenet_V2_model, input_shape=(224, 224, 3), trainable=False)\n",
    "num_of_classes = 2\n",
    "base = tf.keras.applications.MobileNetV2(\n",
    "    include_top=False,\n",
    "    pooling='avg',                # global average pool output\n",
    "    input_shape=(224, 224, 3),\n",
    "    weights='imagenet'\n",
    ")\n",
    "base.trainable = False\n",
    "\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "x = base(inputs, training=False)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(2, activation='softmax')(x) # alternative\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "MxU4QkpBvzIQ"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "JX02Yc2HTLiB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 156ms/step - accuracy: 0.9428 - loss: 0.1489\n",
      "Epoch 2/5\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 173ms/step - accuracy: 0.9832 - loss: 0.0531\n",
      "Epoch 3/5\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 161ms/step - accuracy: 0.9858 - loss: 0.0430\n",
      "Epoch 4/5\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 170ms/step - accuracy: 0.9897 - loss: 0.0308\n",
      "Epoch 5/5\n",
      "\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 173ms/step - accuracy: 0.9913 - loss: 0.0312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x32d6805e0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "g1vDZen5WT2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 168ms/step - accuracy: 0.9803 - loss: 0.0498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.048777107149362564, 0.9806370735168457]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3M2G4Gc9eMgp"
   },
   "source": [
    "#Building a Predictive System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "t7aQacemgnHI"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca20097c5ff64ff9881b3945f0d26440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='image/*', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae8e86df177414e88620309267652ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Single cell: FileUpload widget -> preprocess -> predict -> print Cat/Dog\n",
    "import io\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# -------------------------\n",
    "# REQUIREMENT: Load your model before using the widget.\n",
    "# Example (uncomment + edit path if you need):\n",
    "# import tensorflow as tf\n",
    "# model = tf.keras.models.load_model('/path/to/your_model.h5', compile=False)\n",
    "# -------------------------\n",
    "\n",
    "# Sanity check for model\n",
    "try:\n",
    "    model  # reference to your keras model\n",
    "except NameError:\n",
    "    print(\"Warning: 'model' is not defined. Load your Keras model first (see comment at top of the cell).\")\n",
    "\n",
    "uploader = widgets.FileUpload(accept='image/*', multiple=False)\n",
    "out = widgets.Output()\n",
    "display(uploader, out)\n",
    "\n",
    "def handle_upload(change):\n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "        if not uploader.value:\n",
    "            print(\"No file uploaded yet.\")\n",
    "            return\n",
    "\n",
    "        # Fetch the uploaded file robustly (handles different widget versions)\n",
    "        try:\n",
    "            if isinstance(uploader.value, dict):\n",
    "                fname = list(uploader.value.keys())[0]\n",
    "                fileinfo = uploader.value[fname]\n",
    "            else:\n",
    "                # some versions return a list/tuple\n",
    "                fileinfo = uploader.value[0]\n",
    "                fname = fileinfo.get('name', 'uploaded_image')\n",
    "\n",
    "            print(\"Filename:\", fname)\n",
    "\n",
    "            # Extract bytes\n",
    "            if isinstance(fileinfo, dict) and 'content' in fileinfo:\n",
    "                file_bytes = fileinfo['content']\n",
    "            elif isinstance(fileinfo, (bytes, bytearray)):\n",
    "                file_bytes = fileinfo\n",
    "            else:\n",
    "                # fallback\n",
    "                try:\n",
    "                    file_bytes = io.BytesIO(fileinfo).getvalue()\n",
    "                except Exception as e:\n",
    "                    print(\"Could not extract bytes from upload:\", e)\n",
    "                    return\n",
    "\n",
    "            # Convert bytes -> cv2 image (BGR)\n",
    "            np_arr = np.frombuffer(file_bytes, np.uint8)\n",
    "            img = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\n",
    "            if img is None:\n",
    "                print(\"Uploaded file is not a valid image or cv2 failed to decode it.\")\n",
    "                return\n",
    "\n",
    "            # Preprocess to match training (adjust if you used different preprocessing)\n",
    "            img_resized = cv2.resize(img, (224, 224))\n",
    "            img_input = img_resized.astype(np.float32) / 255.0\n",
    "            img_input = np.expand_dims(img_input, axis=0)  # shape (1,224,224,3)\n",
    "\n",
    "            # Run model.predict and handle errors\n",
    "            try:\n",
    "                pred = model.predict(img_input)\n",
    "            except Exception as e:\n",
    "                print(\"Error during model.predict():\", e)\n",
    "                return\n",
    "\n",
    "            pred = np.asarray(pred)\n",
    "            print(\"Raw model output shape:\", pred.shape)\n",
    "\n",
    "            # Decide class -> 0 is Cat, else Dog\n",
    "            # Handle common output shapes:\n",
    "            label_name = \"Unknown\"\n",
    "            score = None\n",
    "\n",
    "            if pred.ndim == 2 and pred.shape[1] > 1:\n",
    "                # e.g., [[0.1, 0.9]]\n",
    "                label = int(np.argmax(pred, axis=1)[0])\n",
    "                score = float(np.max(pred, axis=1)[0])\n",
    "                label_name = \"Cat\" if label == 0 else \"Dog\"\n",
    "\n",
    "            elif pred.ndim == 2 and pred.shape[1] == 1:\n",
    "                # e.g., [[0.8]] (sigmoid score for positive class)\n",
    "                score = float(pred[0,0])\n",
    "                label = int(score >= 0.5)  # 1 -> Dog, 0 -> Cat\n",
    "                label_name = \"Cat\" if label == 0 else \"Dog\"\n",
    "\n",
    "            elif pred.ndim == 1:\n",
    "                # e.g., [0.1, 0.9] or [0.8]\n",
    "                if pred.size > 1:\n",
    "                    label = int(np.argmax(pred))\n",
    "                    score = float(np.max(pred))\n",
    "                    label_name = \"Cat\" if label == 0 else \"Dog\"\n",
    "                else:\n",
    "                    score = float(pred[0])\n",
    "                    label = int(score >= 0.5)\n",
    "                    label_name = \"Cat\" if label == 0 else \"Dog\"\n",
    "\n",
    "            else:\n",
    "                # Unexpected shape\n",
    "                print(\"Unhandled model output shape; raw output printed below.\")\n",
    "                print(pred)\n",
    "                return\n",
    "\n",
    "            score_str = f\"{score:.4f}\" if score is not None else \"N/A\"\n",
    "\n",
    "            # Display the image and the label\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.imshow(cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(f\"Prediction: {label_name} (score={score_str})\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "            print(\"Prediction:\", label_name, f\"(score={score_str})\")\n",
    "\n",
    "        except Exception as exc:\n",
    "            print(\"Unexpected error in upload handler:\", exc)\n",
    "\n",
    "# Attach handler\n",
    "uploader.observe(handle_upload, names='value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
